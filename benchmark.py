import search_algorithm
from level_manager import LevelManager, Level
from game_state import GameState
from copy import deepcopy
import heuristic
import matplotlib.pyplot as plt
import numpy as np
import time
import os
import pandas as pd
import seaborn as sns
from main import run_algorithm

def run_benchmark():
    # Define levels
    level2 = Level(
        GameState(
            tiles={(2, 1): "green", (3, 3): "purple"},
            targets={(3, 0): "green", (3, 1): "purple"},
            blanks=[(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 2), (1, 3), (2, 2), (3, 2)],
            blockers=[(1, 1), (2, 0), (2, 3)],
            size = 4
        ),
        2
    )   

    level3 = Level(
        GameState(
            tiles={(0, 2): "green", (3, 3): "purple"},
            targets={(3, 0): "green", (2, 1): "purple"},
            blanks=[(0, 0), (0, 1), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3), (2, 0), (2, 2), (2, 3)],
            blockers=[(3, 1), (3, 2)],
            size = 4
        ),
        3
    )

    # Create a more challenging level
    level4 = Level(
        GameState(
            tiles={(0, 0): "red", (0, 4): "blue", (4, 0): "green", (4, 4): "yellow"},
            targets={(2, 2): "red", (1, 2): "blue", (3, 2): "green", (2, 3): "yellow"},
            blanks=[(0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 3), (1, 4), 
                   (2, 0), (2, 1), (2, 4), (3, 0), (3, 1), (3, 3), (3, 4), 
                   (4, 1), (4, 2), (4, 3)],
            blockers=[(3, 2), (2, 3)],
            size = 5
        ),
        5
    )

    level_manager = LevelManager({2: level2, 3: level3, 4: level4})
    
    # Define algorithms and heuristics
    algorithms = [
        ("BFS", lambda state: search_algorithm.BFS(deepcopy(state))),
        ("IDS", lambda state, optimal_moves: search_algorithm.IDS(deepcopy(state), optimal_moves)),
        ("Greedy-SumTeleport", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.SumMinMovesTeleport())),
        ("Greedy-MaxTeleport", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.MaxMinMovesTeleport())),
        ("Greedy-SumBlockers", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.SumMinMovesBlockers())),
        ("Greedy-MaxBlockers", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.MaxMinMovesBlockers())),
        ("Greedy-SumConflicts", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.SumMinMovesConflicts())),
        ("Greedy-MaxConflicts", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.MaxMinMovesConflicts()))
    ]
    
    # Run benchmark
    all_metrics = []
    
    for level_idx in level_manager.levels:
        level = level_manager.get_level(level_idx)
        level_name = f"Level {level_idx}"
        print(f"\n===== Running benchmark for {level_name} =====")
        
        for alg_name, alg_factory in algorithms:
            print(f"\nRunning {alg_name} on {level_name}...")
            try:
                # Set a timeout for each algorithm (30 seconds)
                algorithm_instance = alg_factory(level.initial_state) if alg_name != "IDS" else alg_factory(level.initial_state, level.optimal_moves)
                metrics = run_algorithm(alg_name, algorithm_instance, level_name, level.optimal_moves)
                all_metrics.append(metrics)
            except Exception as e:
                print(f"Error running {alg_name} on {level_name}: {e}")
    
    # Create results directory
    if not os.path.exists("results"):
        os.makedirs("results")
    
    # Convert metrics to DataFrame for easier analysis
    df = pd.DataFrame(all_metrics)
    df.to_csv("results/benchmark_results.csv", index=False)
    
    # Generate plots
    generate_plots(df)
    
def generate_plots(df):
    # Time comparison
    plt.figure(figsize=(14, 10))
    sns.barplot(x="level", y="time", hue="algorithm", data=df)
    plt.title("Execution Time by Algorithm and Level")
    plt.ylabel("Time (seconds)")
    plt.yscale("log")  # Log scale for better visualization
    plt.tight_layout()
    plt.savefig("results/time_comparison_detailed.png")
    
    # Memory comparison
    plt.figure(figsize=(14, 10))
    df["memory_mb"] = df["memory"] / (1024 * 1024)  # Convert to MB
    sns.barplot(x="level", y="memory_mb", hue="algorithm", data=df)
    plt.title("Memory Usage by Algorithm and Level")
    plt.ylabel("Memory (MB)")
    plt.tight_layout()
    plt.savefig("results/memory_comparison_detailed.png")
    
    # States generated comparison
    plt.figure(figsize=(14, 10))
    sns.barplot(x="level", y="states", hue="algorithm", data=df)
    plt.title("States Generated by Algorithm and Level")
    plt.ylabel("Number of States")
    plt.yscale("log")  # Log scale for better visualization
    plt.tight_layout()
    plt.savefig("results/states_comparison_detailed.png")
    
    # Solution quality comparison
    plt.figure(figsize=(14, 10))
    sns.barplot(x="level", y="difference", hue="algorithm", data=df)
    plt.title("Solution Quality by Algorithm and Level")
    plt.ylabel("Difference from Optimal Solution")
    plt.tight_layout()
    plt.savefig("results/solution_quality_detailed.png")
    
    # Efficiency comparison (states per second)
    plt.figure(figsize=(14, 10))
    df["states_per_second"] = df["states"] / df["time"]
    sns.barplot(x="level", y="states_per_second", hue="algorithm", data=df)
    plt.title("Algorithm Efficiency (States Processed per Second)")
    plt.ylabel("States per Second")
    plt.tight_layout()
    plt.savefig("results/efficiency_comparison.png")
    
    # Heatmap of normalized metrics
    plt.figure(figsize=(16, 12))
    metrics = ["time", "memory_mb", "states", "difference", "states_per_second"]
    
    # Create a pivot table for the heatmap
    for metric in metrics:
        plt.figure(figsize=(10, 8))
        pivot = df.pivot_table(index="algorithm", columns="level", values=metric)
        
        # Normalize by column (per level)
        normalized_pivot = pivot.div(pivot.max())
        
        sns.heatmap(normalized_pivot, annot=pivot.round(2), cmap="YlGnBu", 
                   linewidths=.5, fmt=".2f", cbar_kws={'label': f'Normalized {metric}'})
        plt.title(f"Normalized {metric} by Algorithm and Level")
        plt.tight_layout()
        plt.savefig(f"results/heatmap_{metric}.png")
    
    print("Detailed benchmark plots saved to the 'results' directory")

if __name__ == "__main__":
    run_benchmark() 
import search_algorithm
from level_manager import LevelManager
from copy import deepcopy
import heuristic
import matplotlib.pyplot as plt
import os
import pandas as pd
import seaborn as sns
import argparse
from main import run_algorithm

def parse_args():
    parser = argparse.ArgumentParser(description='Run benchmarks for Match The Tiles')
    parser.add_argument('--plot', action='store_true', 
                       help='Generate plots from benchmark results')
    return parser.parse_args()

def run_benchmark(args=None):
    level_manager = LevelManager()
    
    # Define algorithms and heuristics
    algorithms = [
        ("BFS", lambda state: search_algorithm.BFS(deepcopy(state))),
        ("IDS", lambda state, optimal_moves: search_algorithm.IDS(deepcopy(state), optimal_moves)),
        ("Greedy-SumTeleport", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.SumMinMovesTeleport())),
        ("Greedy-MaxTeleport", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.MaxMinMovesTeleport())),
        ("Greedy-SumBlockers", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.SumMinMovesBlockers())),
        ("Greedy-MaxBlockers", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.MaxMinMovesBlockers())),
        ("Greedy-SumConflicts", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.SumMinMovesConflicts())),
        ("Greedy-MaxConflicts", lambda state: search_algorithm.GreedySearch(
            deepcopy(state), heuristic.MaxMinMovesConflicts()))
    ]
    
    # Run benchmark
    all_metrics = []
    
    for level_idx in level_manager.levels:
        level = level_manager.get_level(level_idx)
        level_name = f"Level {level_idx}"
        print(f"\n===== Running benchmark for {level_name} =====")
        
        for alg_name, alg_factory in algorithms:
            print(f"\nRunning {alg_name} on {level_name}...")
            try:
                # Set a timeout for each algorithm (30 seconds)
                algorithm_instance = alg_factory(level.initial_state) if alg_name != "IDS" else alg_factory(level.initial_state, level.optimal_moves)
                metrics = run_algorithm(alg_name, algorithm_instance, level_name, level.optimal_moves)
                all_metrics.append(metrics)
            except Exception as e:
                print(f"Error running {alg_name} on {level_name}: {e}")
    
    # Create results directory
    if not os.path.exists("results"):
        os.makedirs("results")
    
    # Convert metrics to DataFrame for easier analysis
    df = pd.DataFrame(all_metrics)
    df.to_csv("results/benchmark_results.csv", index=False)
    
    if args.plot:
        # Generate plots
        generate_plots(df)
    
def generate_plots(df):
    # Time comparison
    plt.figure(figsize=(14, 10))
    sns.barplot(x="level", y="time", hue="algorithm", data=df)
    plt.title("Execution Time by Algorithm and Level")
    plt.ylabel("Time (seconds)")
    plt.yscale("log")  # Log scale for better visualization
    plt.tight_layout()
    plt.savefig("results/time_comparison_detailed.png")
    
    # Memory comparison
    plt.figure(figsize=(14, 10))
    df["memory_mb"] = df["memory"] / (1024 * 1024)  # Convert to MB
    sns.barplot(x="level", y="memory_mb", hue="algorithm", data=df)
    plt.title("Memory Usage by Algorithm and Level")
    plt.ylabel("Memory (MB)")
    plt.tight_layout()
    plt.savefig("results/memory_comparison_detailed.png")
    
    # States generated comparison
    plt.figure(figsize=(14, 10))
    sns.barplot(x="level", y="states", hue="algorithm", data=df)
    plt.title("States Generated by Algorithm and Level")
    plt.ylabel("Number of States")
    plt.yscale("log")  # Log scale for better visualization
    plt.tight_layout()
    plt.savefig("results/states_comparison_detailed.png")
    
    # Solution quality comparison
    plt.figure(figsize=(14, 10))
    sns.barplot(x="level", y="difference", hue="algorithm", data=df)
    plt.title("Solution Quality by Algorithm and Level")
    plt.ylabel("Difference from Optimal Solution")
    plt.tight_layout()
    plt.savefig("results/solution_quality_detailed.png")
    
    # Efficiency comparison (states per second)
    plt.figure(figsize=(14, 10))
    df["states_per_second"] = df["states"] / df["time"]
    sns.barplot(x="level", y="states_per_second", hue="algorithm", data=df)
    plt.title("Algorithm Efficiency (States Processed per Second)")
    plt.ylabel("States per Second")
    plt.tight_layout()
    plt.savefig("results/efficiency_comparison.png")
    
    # Heatmap of normalized metrics
    plt.figure(figsize=(16, 12))
    metrics = ["time", "memory_mb", "states", "difference", "states_per_second"]
    
    # Create a pivot table for the heatmap
    for metric in metrics:
        plt.figure(figsize=(10, 8))
        pivot = df.pivot_table(index="algorithm", columns="level", values=metric)
        
        # Normalize by column (per level)
        normalized_pivot = pivot.div(pivot.max())
        
        sns.heatmap(normalized_pivot, annot=pivot.round(2), cmap="YlGnBu", 
                   linewidths=.5, fmt=".2f", cbar_kws={'label': f'Normalized {metric}'})
        plt.title(f"Normalized {metric} by Algorithm and Level")
        plt.tight_layout()
        plt.savefig(f"results/heatmap_{metric}.png")
    
    print("Detailed benchmark plots saved to the 'results' directory")

if __name__ == "__main__":
    args = parse_args()
    run_benchmark(args)